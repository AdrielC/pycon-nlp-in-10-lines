{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP in 10 lines tutorial notebook\n",
    "##### PyCon 2016 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [spaCy overview](http://spacy.io/docs/#examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load spaCy resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import spacy and English models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading spaCy can take a while, in the meantime here are a few definitions to help you on your NLP journey.\n",
    "\n",
    "#### What are Stop Words?\n",
    "\n",
    "Stop words are the common words in a vocabulary which are of little value when considering word frequencies in text. This is because they don't provide much useful information about what the sentence is telling the reader.\n",
    "\n",
    "Example: _\"the\",\"and\",\"a\",\"are\",\"is\"_\n",
    "\n",
    "#### What is a Corpus?\n",
    "\n",
    "A corpus (plural: corpora) is a large collection of text or documents and can provide useful training data for NLP models. A corpus might be built from transcribed speech or a collection of manuscripts. Each item in a corpus is not necessarily unique and frequency counts of words can assist in uncovering the structure in a corpus.\n",
    "\n",
    "Examples:\n",
    "\n",
    "1. Every word written in the complete works of Shakespeare\n",
    "2. Every word spoken on BBC Radio channels for the past 30 years "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process sentences 'Hello, world. Natural Language Processing in 10 lines of code.' using spaCy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get tokens and sentences\n",
    "\n",
    "#### What is a Token?\n",
    "A token is a single chopped up element of the sentence, which could be a word or a group of words to analyse. The task of chopping the sentence up is called \"tokenisation\".\n",
    "\n",
    "Example: The following sentence can be tokenised by splitting up the sentence into individual words.\n",
    "\n",
    "\t\"Cytora is going to PyCon!\"\n",
    "\t[\"Cytora\",\"is\",\"going\",\"to\",\"PyCon!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get first token of the processed document\n",
    "\n",
    "# Print sentences (one sentence per line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tags\n",
    "\n",
    "#### What is a Speech Tag?\n",
    "A speech tag is a context sensitive description of what a word means in the context of the whole sentence.\n",
    "More information about the kinds of speech tags which are used in NLP can be [found here](http://www.winwaed.com/blog/2011/11/08/part-of-speech-tags/).\n",
    "\n",
    "Examples:\n",
    "\n",
    "1. CARDINAL, Cardinal Number - 1,2,3\n",
    "2. PROPN, Proper Noun, Singular - \"Matic\", \"Andraz\", \"Cardiff\"\n",
    "3. INTJ, Interjection - \"Uhhhhhhhhhhh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each token, print corresponding part of speech tag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual part of speech tagging ([displaCy](https://displacy.spacy.io))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic dependencies\n",
    "\n",
    "#### What are syntactic dependencies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write a function that walk up the syntactic tree of the given token and collects all tokent to the root token (including root token).\n",
    "\n",
    "# For every token in document, print it's tokens to the root\n",
    "    \n",
    "# Print dependency labels of the tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entities\n",
    "\n",
    "#### Named Entities\n",
    "\n",
    "A named entity is any real world object such as a person, location, organisation or product with a proper name. \n",
    "\n",
    "Example:\n",
    "\n",
    "\t1. Barack Obama\n",
    "\t2. Edinburgh\n",
    "\t3. Ferrari Enzo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print all named entities with named entity types\n",
    "\n",
    "doc_2 = nlp(\"I went to Paris where I met my old friend Jack from uni.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun chunks\n",
    "\n",
    "#### What is a Noun Chunk?\n",
    "Noun chunks are the phrases based upon nouns recovered from tokenized text using the speech tags.\n",
    "\n",
    "Example:\n",
    "\n",
    "The sentence \"The boy saw the yellow dog\" has 2 noun objects, the boy and the dog. \n",
    "Therefore the noun chunks will be\n",
    "\n",
    "\t1. \"The boy\"\n",
    "\t2. \"the yellow dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print noun chunks for doc_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For every token in doc_2, print log-probability of the word, estimated from counts from a large corpus \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding / Similarity\n",
    "\n",
    "#### What are Word embeddings?\n",
    "\n",
    "A word embedding is a representation of a word, and by extension a whole language corpus, in a vector or other form of numerical mapping. This allows words to be treated numerically with word similarity represented as spatial difference in the dimensions of the word embedding mapping.\n",
    "\n",
    "Example:\n",
    "\t\n",
    "With word embeddings we can understand that vector operations describe word similarity. This means that we can see vector proofs of statements such as:\n",
    "\n",
    "\tking-queen==man-woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For a given document, caclulate similarity between 'apples' and 'oranges' and 'boots' and 'hippos'\n",
    "doc = nlp(\"Apples and oranges are similar. Boots and hippos aren't.\")\n",
    "apples = doc[0]\n",
    "oranges = doc[2]\n",
    "boots = doc[6]\n",
    "hippos = doc[8]\n",
    "\n",
    "# Print similarity between sentence and word 'fruit'\n",
    "apples_sent, boots_sent = doc.sents\n",
    "fruit = doc.vocab['fruit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Matplotlib Jupyter HACK\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real text analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process `text` with Spacy NLP Parser\n",
    "text = read_file('data/pride_and_prejudice.txt')\n",
    "processed_text = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many sentences are in Pride & Prejudice book?\n",
    "\n",
    "# Print sentences from index 10 to index 15, to make sure that we have parsed correct book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all the personal names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract all the personal names from Pride & Prejudice and count theirs occurences. \n",
    "# Expected output is a list in the following form: [('elizabeth', 622), ('darcy', 312), ('jane', 286), ('bennet', 266) ...].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot actors personal names as a time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot actor mentions as a time series relative to the position of the actor's ocurence in a book.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy parse tree in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find words (adjectives) that describe Mr Darcy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find actors that are 'talking', 'saying', 'doing' the most. Find the relationship between \n",
    "# entities and corresponding root verbs.\n",
    "\n",
    "# Find all the actors that got married in the book\n",
    "# Some sentence from which information could be extracted\n",
    "# \n",
    "# her mother was talking to that one person (Lady Lucas) freely,\n",
    "# openly, and of nothing else but her expectation that Jane would soon\n",
    "# be married to Mr. Bingley.\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Keywords using noun chunks from the news article (file 'article.txt').\n",
    "# Spacy will pick some noun chunks that are not informative at all (e.g. we, what, who).\n",
    "# Try to find a way to remove non informative keywords.\n",
    "\n",
    "article = read_file('data/article.txt')\n",
    "doc = nlp(article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
