{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pride & Prejudice analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real text analysis\n",
    "\n",
    "We got familiar with Spacy. In the next section we are going to analyse a real text (Pride & Prejudice). \n",
    "\n",
    "We would like to:\n",
    "* Extract all the characters from the book (e.g. Elizabeth, Barcy, Bingley)\n",
    "* Visualize characters' occurences with regards to relative position in the book\n",
    "* Authomatically describe any character from the book\n",
    "* Find out which characters have been mentioned in a context of the marriage\n",
    "* Build keywords extraction that could to display word cloud ([example](http://www.cytora.com/data-samples.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        return file.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Process `text` with Spacy NLP Parser\n",
    "text = read_file('data/pride_and_prejudice.txt')\n",
    "processed_text = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many sentences are in Pride & Prejudice book?\n",
    "sentences = [s for s in processed_text.sents]\n",
    "print(len(sentences))\n",
    "\n",
    "# Print sentences from index 10 to index 15, to make sure that we have parsed correct book\n",
    "print(sentences[10:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all the personal names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract all the personal names from Pride & Prejudice and count their occurrence. \n",
    "# Expected output is a list in the following form: [('elizabeth', 622), ('darcy', 312), ('jane', 286), ('bennet', 266) ...].\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def find_character_occurences(doc):\n",
    "    \"\"\"\n",
    "    Return a list of actors from `doc` with corresponding occurences.\n",
    "    \n",
    "    :param doc: Spacy NLP parsed document\n",
    "    :return: list of tuples in form\n",
    "        [('elizabeth', 622), ('darcy', 312), ('jane', 286), ('bennet', 266)]\n",
    "    \"\"\"\n",
    "    \n",
    "    characters = Counter()\n",
    "    for ent in processed_text.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            characters[ent.lemma_] += 1\n",
    "            \n",
    "    return characters.most_common()\n",
    "\n",
    "print(find_character_occurences(processed_text)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot characters personal names as a time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Matplotlib Jupyter HACK\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot characters mentions as a time series relative to the position of the actor's occurrence in a book.\n",
    "\n",
    "def get_character_offsets(doc):\n",
    "    \"\"\"\n",
    "    For every character in a `doc` collect all the occurences offsets and store them into a list. \n",
    "    The function returns a dictinary that has actor lemma as a key and list of occurences as a value for every character.\n",
    "    \n",
    "    :param doc: Spacy NLP parsed document\n",
    "    :return: dict object in form\n",
    "        {'elizabeth': [123, 543, 4534], 'darcy': [205, 2111]}\n",
    "    \"\"\"\n",
    "    \n",
    "    character_offsets = defaultdict(list)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            character_offsets[ent.lemma_].append(ent.start)\n",
    "            \n",
    "    return dict(character_offsets)\n",
    "\n",
    "character_occurences = get_character_offsets(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import hist\n",
    "from cycler import cycler\n",
    "\n",
    "NUM_BINS = 10\n",
    "\n",
    "def normalize(occurencies, normalization_constant):\n",
    "    return [o / float(len(processed_text)) for o in occurencies]\n",
    "\n",
    "def plot_character_timeseries(character_offsets, character_labels, normalization_constant=None):\n",
    "    \"\"\"\n",
    "    Plot characters' personal names specified in `character_labels` list as time series.\n",
    "    \n",
    "    :param character_offsets: dict object in form {'elizabeth': [123, 543, 4534], 'darcy': [205, 2111]}\n",
    "    :param character_labels: list of strings that should match some of the keys in `character_offsets`\n",
    "    :param normalization_constant: int\n",
    "    \"\"\"\n",
    "    x = [character_offsets[character_label] for character_label in character_labels] \n",
    "        \n",
    "    with plt.style.context('fivethirtyeight'):\n",
    "        plt.figure()\n",
    "        n, bins, patches = plt.hist(x, NUM_BINS, label=character_labels)\n",
    "        plt.clf()\n",
    "        \n",
    "        ax = plt.subplot(111)\n",
    "        for i, a in enumerate(n):\n",
    "            ax.plot([float(x) / (NUM_BINS - 1) for x in range(len(a))], a, label=character_labels[i])\n",
    "            \n",
    "        matplotlib.rcParams['axes.prop_cycle'] = cycler(color=['r','k','c','b','y','m','g','#54a1FF'])\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "#plot_character_timeseries(character_occurences, ['darcy', 'bingley'], normalization_constant=len(processed_text))\n",
    "plot_character_timeseries(character_occurences, ['darcy', 'bingley'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy parse tree in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find words (adjectives) that describe Mr Darcy.\n",
    "\n",
    "def get_character_adjectives(doc, character_lemma):\n",
    "    \"\"\"\n",
    "    Find all the adjectives related to `character_lemma` in `doc`\n",
    "    \n",
    "    :param doc: Spacy NLP parsed document\n",
    "    :param character_lemma: string object\n",
    "    :return: list of adjectives related to `character_lemma`\n",
    "    \"\"\"\n",
    "    \n",
    "    adjectives = []\n",
    "    for ent in processed_text.ents:\n",
    "        if ent.lemma_ == character_lemma:\n",
    "            for token in ent.subtree:\n",
    "                if token.pos_ == 'ADJ': # Replace with if token.dep_ == 'amod':\n",
    "                    adjectives.append(token.lemma_)\n",
    "    \n",
    "    for ent in processed_text.ents:\n",
    "        if ent.lemma_ == character_lemma:\n",
    "            if ent.root.dep_ == 'nsubj':\n",
    "                for child in ent.root.head.children:\n",
    "                    if child.dep_ == 'acomp':\n",
    "                        adjectives.append(child.lemma_)\n",
    "    \n",
    "    return adjectives\n",
    "\n",
    "print(get_character_adjectives(processed_text, 'darcy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find characters that are 'talking', 'saying', 'doing' the most. Find the relationship between \n",
    "# entities and corresponding root verbs.\n",
    "\n",
    "character_verb_counter = Counter()\n",
    "VERB_LEMMA = 'say'\n",
    "\n",
    "for ent in processed_text.ents:\n",
    "    if ent.label_ == 'PERSON' and ent.root.head.lemma_ == VERB_LEMMA:\n",
    "        character_verb_counter[ent.text] += 1\n",
    "\n",
    "print(character_verb_counter.most_common(10)) \n",
    "        \n",
    "# Find all the characters that got married in the book\n",
    "#\n",
    "# Here is an example sentence from which this information could be extracted:\n",
    "# \n",
    "# \"her mother was talking to that one person (Lady Lucas) freely,\n",
    "# openly, and of nothing else but her expectation that Jane would soon\n",
    "# be married to Mr. Bingley.\"\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Keywords using noun chunks from the news article (file 'article.txt').\n",
    "# Spacy will pick some noun chunks that are not informative at all (e.g. we, what, who).\n",
    "# Try to find a way to remove non informative keywords.\n",
    "\n",
    "article = read_file('data/article.txt')\n",
    "doc = nlp(article)\n",
    "\n",
    "keywords = Counter()\n",
    "for chunk in doc.noun_chunks:\n",
    "    if nlp.vocab[chunk.lemma_].prob < - 8: # probablity value -8 is arbitrarily selected threshold\n",
    "        keywords[chunk.lemma_] += 1\n",
    "\n",
    "keywords.most_common(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
